AF_INET = IP4 , AF_INET6 = IP6
SOCK_STREAM = TCP , SOCK_DGRAM = UDP
Server calls = socket , bind , listen , accept, send
Client calls = socket , connect , send , recv
socket setsockopt (options like reuse address etc.), settimeout, setblocking are the main option settings.
socket can exhibit file like behaviour by executing s.makefile() function- form experience resource management sux here and mysterious behaviour exhibited.                                
Utility functions : socket.gethostname(), gethostbyname(), gethostbyaddr(), settimeout(5.0) - setting timout for client on server side to close conn if cliet does not send any data for x secs , setblocking(False) - this raises exception when the socket is blocked
socket call options: , s.setsocketopts(SOL_SOCKET, SO_RESUSEADDR...)
TCP = Connection oriented, UDP = singe datagram , UDS = using files
socket.sendall() waits /blocks until all data is sent.
Typical client code as below for low level socket operation.
chunks = []
while True:
        chunk = s.recv(16384)
        if not chunk: break
        chunks.append(chunk)
 s.close()
response = "".join(chunks)

For UDP s.recvfrom and c.sendto are the two imp methods .No connections just data packets flow.
For Unix Domain sockets for bind and connect method it uses a file 
For ex:s.bind('/tmp/fil.txt') and s.connect('/tmp/fil.txt')

URLLIB:
fields = { 'x':1,'y':2}
parms = urllib.urlencode(fields) # URL encoded string fo parameters
u = urllib.urlopen("http://somedomain.com/subscribe?"+parms) # this is for GET
u = urllib.urlopen("http://somedomain.com/subscribe?"+parms) # this is for POST
u.code , u.headers.items()  important methods
urllib works for simple cases and does not support authentication and cookies and it supports only GET and POST
urllib2 supports cookies and better error handling i,e it raises exceptions for http errors
opener = urllib2.build_opener(urllib2.HTTPCookieprocessor) can be used to extend functionality of urllib2, few others that can be used are ProxyHandler, CacheFTPHandler,AuthHandler
Bottomline: urllib is useful for file fetching or calling webservice with parameters only.
For Finer control with urllib2 we need to use Request class of urllib2 , for ex if we need to add headers and custom data
Ex:
request = urllib2.Request('http://localhost:8080/')
print 'Request method before data:', request.get_method()

request.add_data(urllib.urlencode(query_args))
print 'Request method after data :', request.get_method()
request.add_header('User-agent', 'PyMOTW (http://www.doughellmann.com/PyMOTW/)')
 u = opener.open(request)
resp  = u.read()


f = ftplib.FTP(host,user,pass)
f.retrlines or f.storbinary are methods for operations

h = httplib.HTTPConnection("host:port")
h.request("GET|HEAD|POST" ,"index.html")
d = h.getresponse()
data = d.read(), d.

CacheControl header which has value in time seconds  in http response is used to determine by http client whether to hit the server or its cache for repetitive content.
Last modified(Server) Checking header and If-Modified-Since(client) header used to reduce load on servers. If content is modified then HTTP 200 is returned else HTTP 304 is returned.
ETag(Server) and If-None-Match(client) same functionality  as above.
Accept-Encoding and Content-Encoding used to agree compression and the type of algorithm between client and server 
httplib2 better than httplib and urllib as it supports cache, etag,last modified and compression.
Syntax:
import httplib2
h = httplib2.Http('cache')  #u can use  disable_ssl_certificate_validation=True if needed
resp, content =h.request('url= http://google.com', header={'x':y},ca_certs='/')
resp = response headers , cont = full content

*REQUESTS*
import requests
r = requests.get(url,data,headers,files) 
requests.get('https://github.com', verify=True) # for enabling https
 r = requests.put("http://httpbin.org/put")
 r = requests.delete("http://httpbin.org/delete")
 r = requests.head("http://httpbin.org/get")
 r = requests.options("http://httpbin.org/get")
 
 Consistent calls for all http methods as shown above.
 
Below is how you send parametrs in a GET http call
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("http://httpbin.org/get", params=payload)

print r.url wll show the actual get url
http://httpbin.org/get?key2=value2&key1=value1

print r.text shoes the response that we got back from the server.(Requests will automatically decode content from the server. Most unicode charsets are seamlessly decoded)
r.encoding = 'ISO-8859-1' # This is how u can change the encoding of the response and then access r.text.
For example, HTTP and XML have the ability to specify their encoding in their body. In situations like this, you should use r.content to find the encoding, and then set r.encoding. This will let you use r.text with the correct encoding.

--Binary response Content can be used by r.content
*Json data can be directly decoded by using r.json()

Custome headers are sent as below:
>>> import json
>>> url = 'https://api.github.com/some/endpoint'
>>> payload = {'some': 'data'}
>>> headers = {'content-type': 'application/json'}

>>> r = requests.post(url, data=json.dumps(payload), headers=headers)
data arguement in post can be form encoded in which case u need to send a dict or a string as showsn in example above

Posting a multipart-encoded file.
>>> url = 'http://httpbin.org/post'
>>> files = {'file': open('report.xls', 'rb')}
>>> r = requests.post(url, files=files)

You can set the filename, content_type and headers explicitly:
files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}
r.status_code give the response code.
r.headers gives the respose headers
r.cookies gets the cookies from response.

to send cookies u do below
>>> cookies = dict(cookies_are='working')
>>> r = requests.get(url, cookies=cookies)

r.history gives the history of redirects etc.

requests.get('http://github.com', timeout=0.001) # time to wait by client when server dosent respond	

In the event of a network problem (e.g. DNS failure, refused connection, etc), Requests will raise a ConnectionError exception.
In the rare event of an invalid HTTP response, Requests will raise an HTTPError exception.
If a request times out, a Timeout exception is raised.
If a request exceeds the configured number of maximum redirections, a TooManyRedirects exception is raised.
All exceptions that Requests explicitly raises inherit from requests.exceptions.RequestException.


Session Objects:
The Session object allows you to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance.
A Session object has all the methods of the main Requests API.
Typical Session call:
s = requests.Session()
s.auth = ('user', 'pass')
s.headers.update({'x-test': 'true'})
# both 'x-test' and 'x-test2' are sent
s.get('http://httpbin.org/headers', headers={'x-test2': 'true'})

r.request.headers gives the headers sent in request to the server







Imp difference : SOAP uses xml on http to formulate request and call individual methods associated with the webservice.
REST just calls URL with arguments

lxml,beautifulsoup are libraries used for parsing xml and html

doc = json.load(f) - loads a file into dict and json.dump(doc, f) stores the dict into a file

from string import Template -- this template can be used to replace keys with values in files or stream
d = {'six':6}
Template(open('sim.txt').read()).substitute(d).strip()

Customised HTTP Server: 
from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
Inherit from BaseHTTPRequestHandler  and override the do_GET,do_POST methods etc.
class MyHandler(BAseHTTPRequestHandler):
    def do_GET(self):
             self.send_response()
             self.send_header()
             self.end_header()
            self.wfile.write(f.read())  #sends content takes file contents
serv = HTTPServer(('',8080),MyHandler)
serv.serv_forever()

TCPServer -- same concept as above , it needs a requesthandler class and a SockerServer type

from SocketServer import BaseRequestHandler
import datetime
import SocketServer

class Handler1(BaseRequestHandler):
    def handle(self):
        self.request.sendall(str(datetime.datetime.now())+'\n')
        
tcph = SocketServer.TCPServer(("",9099),Handler1)
tcph.serve_forever()

Read the SockServer code comments for excellent explanation on simultaneous multiple clients
To make the above code accessible to multiple clients simultaneously use a Thread or Fork mixins.

Eg: class MyServer(ThreadMixin,TCPServer) : call the TCP __init__
Eg: class MyServer(ForkingMixin,HTTPServer) : call the TCP __init__
OR
Eg :class MyServer(SocketServer.ForkingTCPServer):call Forking __init__

http://kmkeen.com/socketserver/

**XML RPC
#Server Code 
from SimpleXMLRPCServer import SimpleXMLRPCServer
def add(x,y):
return x+y
s = SimpleXMLRPCServer(("",8080))
s.register_function(add)
s.serve_forever()
#Client Code
import xmlrpclib
s = xmlrpclib.ServerProxy("http://localhost:8080")
s.add(3,5)

*XML-RPC assumes all strings are UTF-8 encoded Unicode
* Dont use xml-rpc when large volumes of data needs to be transferred to and fro.

*pickle used for serialisation , dump used to dump the contents of object into a file,socket etc and load used to read it back and convert into python object.
ex: import pickle
l = range(10) ; f = open('ss.txt','a') ; pickle.dump(l,f) # this saves contents of l  in file ss.txt
l = pickle.load(f) # this loads content of f in l python object
|||ly loads and dumps are used to pickle and unpickle contents of objects into strings.
*pickle has security concerns anybody can send arbitrary commands to python server interpretor if they come to understand that the server is receiving pickled objects


