
*struct
The struct module includes functions for converting between strings of bytes and native Python data types such asnumbers and strings.
v=5 ; import struct ; s = struct.Struct('I') #Argument should be format specifier
v2=s.pack(v) # converts into binary
s.unpack(v2) # converts into relevant data type

*shelve
The shelve module can be used as a simple persistent storage option for Python objects when a relational database
is overkillused to internally pickle objects in key value pair .
import shelve
s = shelve.open('test.db')
s['k1'] = range(10)
s.close()
Shelves do not track modifications to volatile objects, by default.To automatically catch changes to volatile objects stored in the shelf, open the shelf with writeback enabled. The
writeback flag causes the shelf to remember all of the objects retrieved from the database using an in-memory cache.
Each cache object is also written back to the database when the shelf is closed.

*pickle/cPickle
pickle module implements an algorithm for turning an arbitrary Python object into a series of bytes. This
process is also called serializing” the object.Once the data is serialized, you can write it to a file, socket, pipe, etc.
In [3]: d
Out[3]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
In [5]: p = pickle.dumps(d)
In [6]: p
Out[6]: '(lp0\nI0\naI1\naI2\naI3\naI4\naI5\naI6\naI7\naI8\naI9\na.'
n [7]: f = pickle.loads(p)
In [8]: f
Out[8]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
OR

f = open('tmp.file','wb+')
pickle.sump(d,f)
f.close()

When working with your own classes, you must ensure that the class being pickled appears in the namespace of the
process reading the pickle. Only the data for the instance is pickled, not the class definition. The class name is used to
find the constructor to create the new object when unpickling




*For large amounts of data, it may be more efficient(coz stored contiguous blocks of memory) to use an array instead of a list, ex: import array ; a = array.array('i',range(10)) #i is code representing integer
Only difference between list and array is array hols only same data type
a.tofile(output.file)    # to read and write directly from array 
a.fromfile(output.file)


*heapq and bisect are used to generate lists in sorted order when the list is changing continuously
In [88]: for i in range(1,20):
    r = random.randint(1,100)
    bisect.insort(l,r) # this inserts the randm number in sorted order.

*list is not thread safe hence use Queue for queue operations even though list can easily simulate a Queue with insert and remove. multiprocessing.Queue even has a version to work with multiple processes.
 Queue.LifoQueue is the stack implementation.
Queue provides   q.task_done() method supposed to be called after get to notify other threads that processing is done for last get.

**Collections : Deque, namedtuple,ordereddict,defaultdict,Counter
*Deque is a double ended  queue with methods like pop,popleft,append,appendleft,extend,extendleft , rotate,rotateleft. Important application undo/redo functionality in applications.
*defaultdict is used to assign and add key value pairs into dict when accessed and key not present in the dict. for ex: d = defaultdict(int); d = {'s1':1} ; accessing d['s2'] adds 0 as value in the dict. We can even have custom methods just like int is being used here.
*A Counter is a container that keeps track of how many times equivalent values are added. 
c = collections.Counter('extremely')
In [4]: c
Out[4]: Counter({'e': 3, 'm': 1, 'l': 1, 'r': 1, 't': 1, 'y': 1, 'x': 1})
*An OrderedDict is a dictionary subclass that remembers the order in which its contents are added.
*namedtuple instances are just as memory efficient as regular tuples because they  do not have per-instance dictionaries. 
In [5]: p = collections.namedtuple('p','name age')
In [6]: p1 = p('simon',30)



*The tempfile module provides several functions for creating filesystem resources  ecurely. TemporaryFile() opens and returns an un-named file,  NamedTemporaryFile()  , t1 = tempfile.NamedTemporaryFile()

*shutil.copyfile , copyfileobject, copy , copy2 used to make a copy of the file. 
move to move file 
copystat copies all stats from f1 to f2 , copymode copies permission.
copytree , rmtree - recursive directory operations

*dircache provided listdir method to list the contents of directory , it also caches the entries for next time use. .
if the timstamp in dir changes only then it updates the cache. We can manually reset cache by using dircache.reset()


***Pg.11 - Types of Exceptions  and Warning *** Must Read ***

Unicode ; https://docs.python.org/2/howto/unicode.html
Theory:
'''
BOM = Byte order mark
255254 at the beginning means little endian
254255 means big endian
UTF-8 can go upto 8 bytes . If the 1st bit is 0 then its a 1 byte character.The 1st  byte is called leading byte and subsequent bytes called continuation bytes. Leading byte gives the info on how many continuation bytes  needs to be read. For example if 1st byte has 110XXXXX 10XXXXXX then the number of 1's in 1st byte show the number of bytes ie 2.
Unicode != UTF-8 , Unicode is mapping from symbol to codepoint. UTF-8 is the binary representation of the codepoint.
So when u want to convert from bytes to codepoints we decode the stream and when we need to convert from codepoints to bytes we encode them.
Any bytesstring be it file or network stream needs to be decoded into Unicode before playing with the data in your program and for this u need to know the encoding of the data that is coming in. And similarly when u want to send the data out to network or file or databse make sure  u encode the data into utf-8 before saving
Decode as early as possible , Unicode everywhere in your program with 'u' and encode into Bytes as late as possible'''

Fact of Life #1 : Computers are built on  only bytes , Files, Networks everything
In python str: sequence of bytes (2.x default)
unicode : sequence on code points (3.x default)
unicode.encode --> bytes
bytes.decode -->unicode

Python implicitly converts str to unicode during operations lije below
s="Hello" + u"World"
s=u"Hello World" , it does so by "World".decode('ascii')
The above implicit conversin causes issues most of the times.

One-character Unicode strings can also be created with the unichr()
built-in function, which takes integers and returns a Unicode string of length 1
that contains the corresponding code point.  The reverse operation is the
built-in ord() function that takes a one-character Unicode string and
returns the code point value
In [62]: unichr(1234)
Out[62]: u'\u04d2'
In [63]: ord(u'\u04d2')
Out[63]: 1234
*\x takes 2 hex digits , \u 4 and \U 8 

*library python-ftfy fixes mojibake o garbage mis encoded text for u 
*decode translates str to unicode instance, encode  does vice versa. str uses 8 bytes to represet a char where as for unicode it can differ from 8 to 32.
**In general, all text data needs to be decoded from its byte representation as it is read, and encoded from the internal values to a specific representation as it is written. codecs module  provides classes that manage the data encoding and decoding for you,
** codecs.open automatically manages this conversion.

Only when a tty is associated with sys.stdout default encoding is set, if there is no terminal asociated its assumed that encoding will be set manually.
So if not done then pipe's will fail with unicode exception when used as output.
To overcome we have to assign getwriter to sys.stdout as shown below
sys.stdout = codecs.get_writer(encoding)(sys.stdout)
You can get the default locale by using below
lang, encoding = locale.getdefaultlocale()
Same needs to be done for sys.stdin when input is not coming from console or if it is coming from a pipe.

codecs.EncodedFile() can be used to read and write to files if the data encoding is different and the file needs to be written in different encoding.
encoded_file = codecs.EncodedFile(buffer, data_encoding=’utf-8’,
file_encoding=’utf-16’)

# Set standard output encoding to UTF-8.
sys.stdout = codecs.getwriter(’UTF-8’)(sys.stdout) #needed when printing text outside asci

*difflib module used to compare sequences and find differences , output more or less similar to Unix diff command.
import difflib ; d = difflib.ndiff(text1,text2)
get close match works like spell checker google search
In [114]: difflib.get_close_matches('prit',keyword.kwlist)
Out[114]: ['print']


*import string ; string.ascii , string.digits etc have a lot of constants which can be used instead of manually declaring.
string.capwords capitalises all words in a sentence.
string.translate can be used for translations 
import string;leet = string.maketrans(’abegiloprstz’, ’463611092572’) s = ’The quick brown fox jumped over the lazy dog.’
print s.translate(leet)
string.Template can be used to for text substituin in static files like xml , u can inherit this class even and specify your own delimiter etc.
t = string.Template("template cntent $var")
d = {'var':'value'}
t.safe_substitute(d) #This will replace var with value in template.

*StringIO and cStringIO – Work with text buffers using file-like API
This module implements an in-memory file object. This object can be used as input or output to most functions that expect a standard file object.
The StringIO class implements memory file versions of all methods available for built-in file objects, plus a getvalue method that returns the internal string value


*Regular Expressions
m = re.search(pattern,string) -- This returns the match object with m.string having string and m.re.pattern havgin matched pattern .
pattern mentioned above can be compiled as well before hand which reduces runtime overhead
ex: regex = re.compile(pattern ; m = regex.search(string))
By pre-compiling any expressions your module uses when the module is loaded you shift the compilation work to application startup time, instead of a point where the program is responding to a user action.
re.findall(pattern,string) finds all patterns and returns a string of pattern matched , finditerreturns match object smilarly
non greedy match means the matching stops as soon as the match is found , it does not go on to scan the whole string .
match.groups, match.group can be used to fetch specific groups that have matched
compile function has options like re.IGNORECASE, re.MULTILINE ,re.UNICODE
Regular expression testing tool mentioned on page 89 -- Very useful

*textwrap.fill and textwrap.dedent used for wrapping and unindenting 

*datetime:
import time ; time.ctime => onvert a time in seconds since the Epoch to a string in local time
In [116]: time.time()
Out[116]: 1407483165.089312
In [117]: time.ctime(time.time())
Out[117]: 'Fri Aug  8 13:02:51 2014'

print datetime.datetime.now().ctime() #Gives current time and converts into ctime
Setting date : datetime.date(2008, 3, 12)
Setting time : datetime.time(1, 3, 12)
You can use datetime to perform basic arithmetic on date values via the timedelta clas
today = datetime.date.today()
one_day = datetime.timedelta(days=1)
yesterday = today - one_day
print ’Yesterday:’, yesterday
Normal comparison operators >,< can be used to comparre date and time objects
datetime.datetime.now().time() or .day or .date or .second etc gives current values
You can even change the format in whic daatimetime is reported 
format = "%a %b %d %H:%M:%S %Y"
today = datetime.datetime.today()
print ’strptime:’, today.strftime(format)

*dateutil is an excellent 3rd party module which extends datetime and provides easy to use relateiv api's

*time
time.time gives epoch time -- we need time.ctime to convert it into normal time
The time module defines struct_time for holding date ,and time values with components broken out so they are easy to access.
time.localtime() gives localtime broken down
time.gmtime() gives utc time,
time.mktime converts the above locatime (struct_time) into epoch time.

now = time.ctime()  # this is str
parsed = time.strptime(now) # converts it into struct_time
print parsed
print time.strftime("%a %b %d %H:%M:%S %Y", parsed) -- converts the struct_time to formatted string


time.tzname gives the current time zone.
time.tzset sets the current timezone after u set the TZ environment variable






*DBM style databases are databses with key value pair sorga only.

*SQLITE3
1.) Create a db file using connect function if it doestn exist
with sqlite3.connect('sqllte.db') as conn;
    conn.executescript(script_content_read_into_file)
    conn.execute(insert into project (name, description, deadline)
    values (’pymotw’, ’Python Module of the Week’, ’2010-11-01’))
2.)Querying is a two step process. First, run the query with the cursor’s execute() method to tell the database engine
what data to collect. Then, use fetchall() to retrieve the results. The return value is a sequence of tuples containing
the values for the columns included in the select clause of the query
cur  = conn.cursor() ; cur.execute('select * from task') ; for r in cur.fetchall():print r - THis will return tuples with data values
To get column info - > for colinfo in cursor.description: print colinfo

Connection objects have a row_factory property that allows the calling code to control the type of object created to represent each row in the query result set.  Row instances can be accessed by column index and name, example below:
cur = conn.cursor(); cur.row_factory= sqlite.Row ; 
cur.execute('select * from task'); for r in cur.fetchall(): print r['id'], r['name'] # u see the references are being made by column names

Use variables to prevent SQL injection attacks : ecample for named variables/
query = "select * from task where id = :id"
cur.execute(query,{'id':123}) or u can use a variable as well for 123 which is declared in advance.
Query parameters can be used with select, insert, and update statements. They can appear in any part of the query where a literal value is legal.
To apply the same SQL instruction to a lot of data use executemany(). This is useful for loading data, since it avoids looping over the inputs in Python and lets the underlying library apply loop optimizations.Example below:
SQL = "insert into task (details, priority, status, deadline, project) values (:details, :priority, 'active', :deadline, :project)"
d = csv.DictReader(open('sam.csv'))
cur.executemany(SQl,d)
d looks somehitng like below:
In [38]: for i in d:
    print i
{'project': 'pymotw', 'priority': '2', 'deadline': '2010-10-02', 'details': 'finish reviewing markup'}
{'project': 'pymotw', 'priority': '2', 'deadline': '2010-10-03', 'details': 'revise chapter intros'}
{'project': 'pymotw', 'priority': '1', 'deadline': '2010-10-03', 'details': 'subtitle'}

*Conversion for types beyond those supported by default is enabled in the database connection using the detect_types flag. Use PARSE_DECLTYPES is the column was declared using the desired type when the table was defined.
with sqlite3.connect(db_filename, detect_types=sqlite3.PARSE_DECLTYPES) as conn:
To open an in-memory database, use the string ’:memory:’ instead of a filename when creating the Connection.
with sqlite3.connect(’:memory:’) as conn: 


**TO BE CONTINUED **


Threading : 
Basic structure for threading.
def worker(val):pass 
for i in range(5):
t=threading.Thread(name="Anython", target=worker,args=(val,)) 
t.start() #Starts 5 threads
The logging module supports embedding the thread name in every log message using the formatter code %(threadName)s

logging.basicConfig(level=logging.DEBUG,format='[%(levelname)s] (%(threadName)-10s) %(message)s',)
logging is also thread-safe, so messages from different threads are kept distinct in the output.

*Up to this point, the example programs have implicitly waited to exit until all threads have completed their work.
Sometimes programs spawn a thread as a daemon that runs without blocking the main program from exiting.
Waiting for the daemon thread to exit using join() means it has a chance to produce its "Exiting" mess
If you for example want to concurrently download a bunch of pages to concatenate them into a single large page you may start concurrently downloads using threads, but need to wait until the last page/thread is finished before you start assembling a single page out of many. That's when you use join()
Basically main thread or the thread from where u make join call  waits until the thread that join is called upon completes execution.
threading.enumerate holds list of active threads , this can be looped into and joined , care should be taken to skip current thread

Basic Structre #2 for Threads -- this is how we define custom threads
class MyThread(threading.Thread):
def run(self):	#Overrding run method and putting in business in runmethod
	logging.debug(’running’)
	return

for i in range(5):
t = MyThread()
t.start()


Because the args and kwargs values passed to the Thread constructor are saved in private variables, they are not
easily accessed from a subclass. To pass arguments to a custom thread type, redefine the constructor to save the values
in an instance attribute that can be seen in the subclass

class MyThreadWithArgs(threading.Thread):
def __init__(self, group=None, target=None, name=None,args=(), kwargs=None, verbose=None):
	threading.Thread.__init__(self, group=group, target=target, name=name,verbose=verbose)
	self.args = args
	self.kwargs = kwargs
	return
def run(self);pass

* simple way to communicate between threads is using threading.Event objects. An Event manages an internal flag that callers can either set() or clear().
Other threads can wait() for the flag to be set(), effectively blocking progress until allowed to continue (Pg. 451)

*t = threading.Lock() can be used to synchronize between threads and work on common resources.
t.acquire() and t.release() are the associated methods.
Normal Lock objects cannot be acquired more than once, even by the same thread so its better to use threading.RLock()# i,e reentrant locks
Context managers can be used for acquiring and releasing locks , snippet below:
def worker_with(lock):
	with lock:
		logging.debug(’Lock acquired via with’)
*In addition to using Events, another way of synchronizing threads is through using a Condition object. Because
the Condition uses a Lock, it can be tied to a shared resource
def consumer(cond):
	logging.debug(’Starting consumer thread’)
	t = threading.currentThread()
	with cond:
		cond.wait()
		logging.debug(’Resource is available to consumer’)

def producer(cond):
	logging.debug(’Starting producer thread’)
	with cond:
		logging.debug(’Making resource available’)
		cond.notifyAll()

condition = threading.Condition()
c1 = threading.Thread(name=’c1’, target=consumer, args=(condition,))
c2 = threading.Thread(name=’c2’, target=consumer, args=(condition,))
p = threading.Thread(name=’p’, target=producer, args=(condition,))

use threading.Sepaphore(number of threads) when u want to give simultaneous accesss to a specific number of threads on a shared resource.

*MULTIPROCESSING
Below structere same as threading structure.
import multiprocessing
def worker():
	"""worker function"""
	print ’Worker’
	return

if __name__ == ’__main__’:
jobs = []
for i in range(5):
p = multiprocessing.Process(target=worker)
jobs.append(p)
p.start()

Unlike with threading, to pass arguments to a multiprocessing Process the argument must be able to be serialized
using pickle which makes sense as data might flow between processes.
One difference between the threading and multiprocessing examples is the extra protection for __main__
used in the multiprocessing examples. Due to the way the new processes are started, the child process needs
to be able to import the script containing the target function. Wrapping the main part of the application in a check
for __main__ ensures that it is not run recursively in each child as the module is imported

*name = multiprocessing.current_process().name -- to get the name of current process
same usage of join as in threading.
*p.terminate() kills the process 
It is important to join() the process after terminating it in order to give the background machinery time to
update the status of the object to reflect the termination.

Below method can be used to set loggin for mutiprocessing lines of code
multiprocessing.log_to_stderr()
logger = multiprocessing.get_logger()
logger.setLevel(logging.INFO)
The other method of subclassing and creating processes can also be used.

Using muiltple processes to pass data around using queues
import multiprocessing
class MyFancyClass(object):
	def __init__(self, name):
		self.name = name

	def do_something(self):
		proc_name = multiprocessing.current_process().name
		print ’Doing something fancy in %s for %s!’ % (proc_name, self.name)

def worker(q):
	obj = q.get()
	obj.do_something()


if __name__ == ’__main__’:
	queue = multiprocessing.Queue()
	p = multiprocessing.Process(target=worker, args=(queue,))
	p.start()
	queue.put(MyFancyClass(’Fancy Dan’))
	queue.close()
	queue.join_thread()
	p.join()


***multiprocessing Lock , Semaphore and Event work the same way as they do in for threads ***

Managiing Shared state
import multiprocessing
def worker(d, key, value):
	d[key] = value

if __name__ == ’__main__’:
mgr = multiprocessing.Manager()
d = mgr.dict()
jobs = [ multiprocessing.Process(target=worker, args=(d, i, i*2)) for i in range(10)]
for j in jobs:
	j.start()
for j in jobs:
	j.join()

print ’Results:’, d
By creating the list through the manager, it is shared and updates are seen in all processes. Dictionaries are also
supported.

*The Pool class can be used to manage a fixed number of workers for simple cases where the work to be done can be
broken up and distributed between workers independently

Ex:
def do_calculation():
	blah blah

pool_size = multiprocessing.cpu_count() * 2 
pool = multiprocessing.Pool(processes=pool_size,initializer=start_process) # This asigns the Pool
pool_outputs = pool.map(do_calculation, inputs) # This picks ans assigns task to processes
pool.close() # no more tasks
pool.join() # wrap up current tasks


*** Map Reduce to be Read **

*FILE AND DIRECTORY ACCESS*

*os.path can be used to split dir and file names file extensions expand env variables, normalise paths, get create modifytime 
os.stat give stats like permission etc abt the file
The os module provides a wrapper for platform specific modules such as posix, nt, and mac.
os.path - os.path.basename('/tmp/sim.txt') returns the file or ex: in sim.txt
os.path - os.path.dirname  returns the dir retrns /tmp
os.path.splitext() works on '.' as separator
os.path.split works on '/' as separator
os.path.join(*parts) -- will build paths from tuples where parts = ('/tmp','sim.txt')
os.path.expandvars() -- expands shell variables present in path
*Paths assembled from separate strings using join() or with embedded variables might end up with extra separators or relative path components. Use normpath() to clean them up
os.path.abspath converts relative path to abspath

Self explanatory methods of os.path
os.path.isabs(file)
print ’Is File?  os.path.isfile(file)
print ’Is Dir?  os.path.isdir(file)
print ’Is Link?  os.path.islink(file)
print ’Mountpoint?  os.path.ismount(file)
print ’Exists? os.path.exists(file)
print ’Link Exists?  os.path.lexists(file)

time.ctime(os.path.getatime(__file__))   #Access time
time.ctime(os.path.getmtime(__file__))   #Modified time
time.ctime(os.path.getctime(__file__))   #Changed time


os.path.walk() traverses all of the directories in a tree and calls a function you provide passing the directory
name and the names of the contents of that directory
Ex:
os.path.walk(’example’, visit, ’(User data)’)

def visit(arg, dirname, names): 
	pass
	
#Here example is the root directory passed , visit is the the  method to be called upon encountering each dir with arg as 'User data'.
When visit is called arg holds 'User data', dirname holds root dir here is example and names holds a tuples with listing of example dir.

os.getenv('PATH') can be used to get evinronment variables values

os.getcwd() -- gets current working directry
os.chdir() -- changes dir
os.pardir() --returns ..

The function access() can be used to test the access rights a process has for a file.
eg:
’Exists:’, os.access(__file__, os.F_OK)
’Readable:’, os.access(__file__, os.R_OK)
’Writable:’, os.access(__file__, os.W_OK)
’Executable:’, os.access(__file__, os.X_OK)

The above return true or false .

stat_info = os.stat('/tmp/sim.txt') # os.stat helps retrieve valuable stats from file as below
’\tSize:’, stat_info.st_size
’\tPermissions:’, oct(stat_info.st_mode)
’\tOwner:’, stat_info.st_uid
’\tDevice:’, stat_info.st_dev
’\tLast modified:’, time.ctime(stat_info.st_mtime)

os.makedirs, os.rmdirs, os.listdir methods for directoy related operations.

os.symlink , os.readlins, os.unlink creates reads and unlinks respectively

The function os.walk() traverses a directory recursively and for each directory generates a tuple containing the directory
path, any immediate sub-directories of that path, and the names of any files in that directory.

Eg : for dir_name, sub_dirs, files in os.walk(root):
print ’\n’, dir_name


os.fork can be used to fork child processes.

*The fileinput module is a framework for creating command line programs for processing text files in a filter-ish manner,
 no need to worry at opening file reading line by line and related error handling. Pg.268 , fileinput.input().
 It takes care of all above. For ex :
 for line in fileinput.input('sim.txt'):
	print lin

fileinput.filelineno() gives the curent line number
Use inplace=True for input method when u want to open files and make inline edins in the file. A tmp.bak file is created while edit 
is in progress .


*tempfile.TemporaryFile() creates a temporarily file with efault premissio w+b , this is unnamed my default.
temp = tempfile.NamedTemporatyFile() creates named temp file , temp.name gives the name
tempfile.mkdtemp() -- creates temp directory
									
By resetting tempfile.tempdir variable you can change temp dir settings where the file gets created.

*GLOB*
Even though the glob API is very simple, the module packs a lot of power. It is useful in any situation where your
program needs to look for a list of files on the filesystem with names matching a pattern.


*fnmatch() compares a single filename against a pattern and returns a boolean indicating whether or not they match.
pattern = ’FNMATCH_*.PY’
fnmatch.fnmatchcase(name, pattern) returns true or false depending on match or not
fnmatch.filter(name,pattern) returns the matched file names

*linecache can be used to extract specific lines from files , it even uses its own cache for efficient retirieval of content .
 linecache.getline('sim.txt', 5)


*getopt , optparse and argparse are 3 apis for command line arguement parsing in the order of theier introduction in stdlib



*SUBPROCESS*
The subprocess module defines one class, Popen and a few wrapper functions that use that class.
subprocess.call(['ls', ' -lrt'],shell=True) used as an alternative for running system commands os.system/
Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to
run the command.The return value from call() is the exit code of the program.
The return value from call() is the exit code of the program. The caller is responsible for interpreting it to detect
errors. The check_call() function works like call() except that the exit code is checked, and if it indicates an
error happened then a CalledProcessError exception is raised.
outp = subprocess.check_output returns the output of the command executed which can be captured

*subprocess.Popen
the below sends data  to output via pipe
proc = subprocess.Popen([’echo’, ’"to stdout"’],stdout=subprocess.PIPE,)
stdout_value = proc.communicate()[0]

The below receives data via pipe
proc = subprocess.Popen([’cat’, ’-’],stdin=subprocess.PIPE,)
proc.communicate(’\tstdin: to stdin\n’)

proc = subprocess.Popen([’cat’, ’-’],stdin=subprocess.PIPE,stdout=subprocess.PIPE,)
stdout_value = proc.communicate(’through stdin to stdout’)[0]


proc = subprocess.Popen(’cat -; echo "to stderr" 1>&2’,shell=True,stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE,)
stdout_value, stderr_value = proc.communicate(’through stdin to stdout’)


*Logging*

Most applications are probably going to want to log to a file. Use the basicConfig() function to set up the default
handler so that debug messages are written to a file.
LOG_FILENAME = ’logging_example.out’
logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG,) #Handler beng set up
logging.debug(’This message should go to the log file’)

Rotating Log File :
my_logger = logging.getLogger(’MyLogger’)
my_logger.setLevel(logging.DEBUG)
# Add the log message handler to the logger
handler = logging.handlers.RotatingFileHandler(LOG_FILENAME,maxBytes=20,backupCount=5,)
my_logger.addHandler(handler)

Level		Value
CRITICAL	50
ERROR	40
WARNING	30
INFO	20
DEBUG	10
UNSET	0

 The log message is only emitted if the handler and
logger are configured to emit messages of that level or higher. For example, if a message is CRITICAL, and the logger
is set to ERROR, the message is emitted (50 > 40). If a message is a WARNING, and the logger is set to produce only
messages set to ERROR, the message is not emitted (30 < 40).
kl
