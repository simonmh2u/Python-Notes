REDIS
Redis supports five different data structures: strings, hashes, lists, sets and ordered sets

Strings:
SET pages:about "about us"
GET pages:about
or 

127.0.0.1:6379> set  number:1 1
OK
127.0.0.1:6379> get number:1
"1"

DEL , TYPE ,EXISTS, INCR , INCRBY , DECR DECRBY , MGET(multiple) , MSET (multiple),GETSET(get old value set new value) are other commands 

> exists mykey
(integer) 1
> set mykey x
OK
> del mykey
(integer) 1

> mset a 10 b 20 c 30
OK
> mget a b c
1) "10"
2) "20"
3) "30

> incr counter
(integer) 102
> incrby counter 50
(integer) 152

> expire key 5 # key gets deleted after 5 seconds
(integer) 1

> set key 100 ex 10 # setting expiry in one command
ttl key is used to find the expory tmie



****List:*****
PUsh commands
> rpush mylist A
(integer) 1
> rpush mylist B
(integer) 2
> lpush mylist first
(integer) 3
> lrange mylist 0 -1  (-1 is the last element index just like pythonlists)
1) "first"
2) "A"
3) "B

On the contrary rpop, lpop are the commnds used to remove elements from cache.

Lists are useful for a number of tasks, two very representative use cases are the following:
# Remember the latest updates posted by users into a social network.
# Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, 
and a consumer (usually a worker) consumes those items executing actions. Redis has special list commands to make 
this use case both more reliable and efficient.

ltrim mylist 0 2 will just trip the list to the specific index size and discard remainging elements.
brpop , blpop are blocking commands i.e they wait on the queue if its empty until a producter pushes somethin onto them.

*When we add an element to an aggregate data type, if the target key does not exist, an empty aggregate data type is crated before
 adding the element and hence subsequent commands shuold match the data type created.

*When we remove elements from an aggregate data type, if the value remains empty, the key is automatically destroyed.
Lists let you associate an array of values to a single key. In fact, you can (and should) think of them as dynamic arrays. You can insert, append, pop, push, trim and so on.


**HASHES***

Hash:The hash data structure is exactly what you are thinking it is (a hash/dictionary).
>hmset user:1000 username antirez birthyear 1977 verified 1
OK
>   user:1000 username
"antirez"

Other commands :
hincrby user:1000 birthyear 10



****Sets:*****
Sets are a lot like lists, except they provide set-semantics (no duplicate values in a given set). You can diff sets via SDIFF, union two sets via SUNION or SUNIONSTORE .
> sadd myset 1 2 3
(integer) 3
> smembers myset
1. 3
2. 1
3. 2

> sismember myset 3  #true if present in set
(integer) 1
> sismember myset 30
(integer) 0

Sets are good for expressing relations between objects
Simple way to model this problem is to have a set for every object we want to tag. The set contains the IDs of the tags associated with the object.
Imagine we want to tag news. If our news ID 1000 is tagged with tag 1,2,5 and 77, we can have one set associating our tag IDs with the news:
> sadd news:1000:tags 1 2 5 77
(integer) 4
> sadd tag:1:news 1000
(integer) 1
> sadd tag:2:news 1000
(integer) 1
> sadd tag:5:news 1000
(integer) 1
> sadd tag:77:news 1000
(integer) 1

sinter tag:1:news tag:2:news tag:10:news tag:27:news # Gives all news items with mentioned tags
Sinter is the intersection between sets.Intersection is not the only operation performed, you can also perform unions, difference, extract a random element, and so forth.
scard returns number if elements in a set.



Sorted Sets;
It's wrong to call one structure more powerful than the others, since they all tend to serve fairly distinct needs,
 but sorted sets are pretty awesome. A sorted set is similar to a set, except each value is associated (and sorted by) a score field.
Sorted sets are a data type which is similar to a mix between asSet and an hash. Like sets, sorted sets are composed of unique, non-repeating string elements, so in some sense a sorted set is a set as well.
> zadd hackers 1940 "Alan Kay"
(integer) 1
> zadd hackers 1957 "Sophie Wilson"
(integer 1)

zrange hackers 0 -1
zrevrange hackers 0 -1
zrange hackers 0 -1 withscores

zrangebyscore hackers -inf 1950
1) "Alan Turing"
2) "Hedy Lamarr"
3) "Claude Shannon"
4) "Alan Kay"
5) "Anita Borg"




Model for Twitter : 

users hash containing userame and password -- when the user is created add entries in this hash.
somethign like 
set user_id 1000
incr user_id
users:user_id username "Simon" password "pass"

following sorted set  -- add the userd ids of users you follow when  you clik the folow button
somethign like
following:user_id time() 999 time() 1023 # time is the score used here
followers sorted set -- add the user ids of users that are following you
following:user_id time() 888 time() 0988 #time is the score

When user posts something by clicking post button we ad it into post hash something like.
post:post_id time time() userid user_id status "Message"

You also go ahead and add this post into posts list of all the users follwing you (called as Fanout)
posts:user_id post_id1,post_id2 ....



REDIS Python Binding :
r = redis.StrictRedis(host='localhost', port=6379, db=0)
r.set('foo','bar')
r.get('foo')

By default, each Redis instance you create will in turn create
its own connection pool. You can override this behavior and use an existing
connection pool by passing an already created connection pool instance to the
connection_pool argument of the Redis class

 >>> pool = redis.ConnectionPool(host='localhost', port=6379, db=0)
 >>> r = redis.Redis(connection_pool=pool)
 
U can use unix domain sockets as below
r = redis.Redis(unix_socket_path='/tmp/redis.sock')


Parser classes provide a way to control how responses from the Redis server
are parsed. redis-py ships with two parser classes, the PythonParser and the
HiredisParser. By default, redis-py will attempt to use the HiredisParser if
you have the hiredis module installed and will fallback to the PythonParser
otherwise

Pipelines are a subclass of the base Redis class that provide support for
buffering multiple commands to the server in a single request. They can be used
to dramatically increase the performance of groups of commands by reducing the
number of back-and-forth TCP packets between the client and server.


    >>> r = redis.Redis(...)
    >>> r.set('bing', 'baz')
    >>> # Use the pipeline() method to create a pipeline instance
    >>> pipe = r.pipeline()
    >>> # The following SET commands are buffered
    >>> pipe.set('foo', 'bar')
    >>> pipe.get('bing')
    >>> # the EXECUTE call sends all buffered commands to the server, returning
    >>> # a list of responses, one for each command.
    >>> pipe.execute()

They can be chained as below:
pipe.set('foo', 'bar').sadd('faz', 'baz').incr('auto_number').execute()


redis-py includes a `PubSub` object that subscribes to channels and listens
for new messages. Creating a `PubSub` object is 
>>> p.subscribe('my-first-channel', 'my-second-channel', ...)
>>> p.psubscribe('my-*', ...)

>>> p.get_message()
{'pattern': None, 'type': 'subscribe', 'channel': 'my-second-channel', 'data': 1L}

>>> r.publish('my-channel','Hellp')
>>> p.get_message()
{'channel': 'my-channel', data': 'Hellp', 'pattern': None, 'type': 'message'}

Useful code block for pubsub
   >>> while True:
    >>>     message = p.get_message()
    >>>     if message:
    >>>         # do something with the message
    >>>     time.sleep(0.001)



Some examples:

>>> import redis
>>> r_server = redis.Redis("localhost")


Data structure: String
>>> r_server.set("name", "DeGizmo")
True
>>> r_server.get("name")
'DeGizmo'

Integer
>>> r_server.set("hit_counter", 1)
True
>>> r_server.incr("hit_counter")
2
>>> r_server.get("hit_counter")
'2'

Lists:
>>> r_server.rpush("members", "Carol")
True
>>> r_server.lrange("members", 0, -1)
['Adam', 'Bob', 'Carol']
>>> r_server.llen("members")
3
>>> r_server.lindex("members", 1)
'Bob'
>>> r_server.rpop("members")
'Carol'
>>> r_server.lrange("members", 0, -1)
['Adam', 'Bob']
>>> r_server.lpop("members")
'Adam'
>>> r_server.lrange("members", 0, -1)
['Bob']


Set;
>>> r_server.sadd("members", "Carol")
True
>>> r_server.sadd("members", "Adam")
False
>>> r_server.smembers("members")
set(['Bob', 'Adam', 'Carol'])


Sorted Sets:

>>> r_server.zadd("stories:frontpage", "storyid:3123", 34)
True
>>> r_server.zadd("stories:frontpage", "storyid:9001", 3)
True
>>> frontpage = r_server.zrange("stories:frontpage", 0, -1, withscores=True)
>>> frontpage.reverse()
>>> frontpage
[('storyid:2134', 127.0), ('storyid:3123', 34.0), ('storyid:9001', 3.0)]


slaveoff is the config used for relpication which , the slave pulls data frmo master ans syncs it self.
usually slave is read only and only master takes writes.




***************CELERY**********************

GEneral Async Use Cases for Celery:
*geenrating assets after upload , like resigin pics etc.
*notifying users when an even t happends like , when somone start to follow you , send email
*Keeping search index uptodate
*replaceing cron jobs.


Mesage broker stores tasks on queues , allows workers on diff machne/proces to produce consume tasks.
The broker stors the data that answers the question ' What work remains to be done ?'
Result Backend: To track state of the task and the return values once the task is complete.

producer --> Broker --> consumer -> workers --> result backend (except broker and result backend all components are part of celery)

Dont pass object instances to calery tasks.
For concurrency use multiprocessnig
profile stuff and break down tasks.




Let’s create the file tasks.py:

from celery import Celery

app = Celery('tasks', broker='amqp://guest@localhost//')

@app.task
def add(x, y):
    return x + y

The first argument to Celery is the name of the current module, this is needed so that names can be automatically generated, the second argument is the broker keyword argument which specifies the URL of the message broker you want to use, using RabbitMQ here,

You now run the worker by executing our program with the worker argument
$ celery -A tasks worker --loglevel=info
In production you will want to run the worker in the background as a daemo

To call our task you can use the delay() method.
>>> from tasks import add
>>> add.delay(4, 4)
Calling a task returns an AsyncResult instance which can be used to check the state of the task, wait for the task to finish or get its return value 


If you want to keep track of the tasks’ states, Celery needs to store or send the states somewhere.
For this example you will use the amqp result backend, which sends states as messages. The backend is specified via the backend argument to Celery, (or via the CELERY_RESULT_BACKEND setting if you choose to use a configuration module):

app = Celery('tasks', backend='amqp', broker='amqp://')

The ready() method returns whether the task has finished processing or not:
>>> result.ready()
False

You can wait for the result to complete, but this is rarely used since it turns the asynchronous call into a synchronous one:
>>> result.get(timeout=1)

If the task raised an exception you can also gain access to the original traceback:
>>> result.traceback



from celery import Celery

app = Celery('proj',
             broker='amqp://',
             backend='amqp://',
             include=['proj.tasks'])

# Optional configuration, see the application user guide.
app.conf.update(
    CELERY_TASK_RESULT_EXPIRES=3600,)

The include argument is a list of modules to import when the worker starts. You need to add our tasks module here so that the worker is able to find our tasks.
The default concurrency number(workers) is the number of CPU’s on that machine (including cores), you can specify a custom number using -c option.


– Events is an option that when enabled causes Celery to send monitoring messages (events) for actions occurring in the worker.

– Queues is the list of queues that the worker will consume tasks from.

Using multi to start workers:

# Single worker with explicit name and events enabled.
$ celery multi start Leslie -E

# Pidfiles and logfiles are stored in the current directory
# by default.  Use --pidfile and --logfile argument to change
# this.  The abbreviation %N will be expanded to the current
# node name.
$ celery multi start Leslie -E --pidfile=/var/run/celery/%N.pid
                                --logfile=/var/log/celery/%N.log


# You need to add the same arguments when you restart,
# as these are not persisted anywhere.
$ celery multi restart Leslie -E --pidfile=/var/run/celery/%N.pid
                                 --logfile=/var/run/celery/%N.log

# To stop the node, you need to specify the same pidfile.
$ celery multi stop Leslie --pidfile=/var/run/celery/%N.pid

# 3 workers, with 3 processes each
$ celery multi start 3 -c 3
celery worker -n celery1@myhost -c 3
celery worker -n celery2@myhost -c 3
celery worker -n celery3@myhost -c 3

# start 3 named workers
$ celery multi start image video data -c 3
celery worker -n image@myhost -c 3
celery worker -n video@myhost -c 3
celery worker -n data@myhost -c 3

# specify custom hostname
$ celery multi start 2 --hostname=worker.example.com -c 3
celery worker -n celery1@worker.example.com -c 3
celery worker -n celery2@worker.example.com -c 3

# specify fully qualified nodenames
$ celery multi start foo@worker.example.com bar@worker.example.com -c 3

# Advanced example starting 10 workers in the background:
#   * Three of the workers processes the images and video queue
#   * Two of the workers processes the data queue with loglevel DEBUG
#   * the rest processes the default' queue.
$ celery multi start 10 -l INFO -Q:1-3 images,video -Q:4,5 data
    -Q default -L:4,5 DEBUG

# You can show the commands necessary to start the workers with
# the 'show' command:
$ celery multi show 10 -l INFO -Q:1-3 images,video -Q:4,5 data
    -Q default -L:4,5 DEBUG

# Additional options are added to each celery worker' comamnd,
# but you can also modify the options for ranges of, or specific workers

# 3 workers: Two with 3 processes, and one with 10 processes.
$ celery multi start 3 -c 3 -c:1 10
celery worker -n celery1@myhost -c 10
celery worker -n celery2@myhost -c 3
celery worker -n celery3@myhost -c 3

# can also specify options for named workers
$ celery multi start image video data -c 3 -c:image 10
celery worker -n image@myhost -c 10
celery worker -n video@myhost -c 3
celery worker -n data@myhost -c 3

# ranges and lists of workers in options is also allowed:
# (-c:1-3 can also be written as -c:1,2,3)
$ celery multi start 5 -c 3  -c:1-3 10
celery worker -n celery1@myhost -c 10
celery worker -n celery2@myhost -c 10
celery worker -n celery3@myhost -c 10
celery worker -n celery4@myhost -c 3
celery worker -n celery5@myhost -c 3

# lists also works with named workers
$ celery multi start foo bar baz xuzzy -c 3 -c:foo,bar,baz 10
celery worker -n foo@myhost -c 10
celery worker -n bar@myhost -c 10
celery worker -n baz@myhost -c 10
celery worker -n xuzzy@myhost -c 3



>>> add.delay(2, 2)

This method is actually a star-argument shortcut to another method called apply_async():
>>> add.apply_async((2, 2))
The latter enables you to specify execution options like the time to run (countdown), the queue it should be sent to and so on:
>>> add.apply_async((2, 2), queue='lopri', countdown=10)

The delay and apply_async methods return an AsyncResult instance, which can be used to keep track of the tasks execution state

*If you have a result backend configured you can retrieve the return value of a task:

>>> res = add.delay(2, 2)
>>> res.get(timeout=1)
4
You can find the task’s id by looking at the id attribute:
>>> res.id
d6b3aea2-fb9b-4ebc-8da4-848818db9114

So how does it know if the task has failed or not? It can find out by looking at the tasks state:
>>> res.state
'FAILURE'

A task can only be in a single state, but it can progress through several states. The stages of a typical task can be:
PENDING -> STARTED -> SUCCESS


The started state is a special state that is only recorded if the CELERY_TRACK_STARTED setting is enabled, or if the @task(track_started=True) option is set for the task.


*Routing*
The CELERY_ROUTES setting enables you to route tasks by name and keep everything centralized in one location:
app.conf.update(
    CELERY_ROUTES = {
        'proj.tasks.add': {'queue': 'hipri'},
    },
)
You can also specify the queue at runtime with the queue argument to apply_async:
>>> from proj.tasks import add
>>> add.apply_async((2, 2), queue='hipri')


*You can then make a worker consume from this queue by specifying the -Q option:
$ celery -A proj worker -Q hipri

Use below to see wht this worker is curretly doing.
*$ celery -A proj inspect active --destination=celery@example.com

*For example you can force workers to enable event messages (used for monitoring tasks and workers):
$ celery -A proj control enable_events
When events are enabled you can then start the event dumper to see what the workers are doing:
$ celery -A proj events --dump


There are several options you can set that will change how Celery works. These options can be set directly on the app instance, or you can use a dedicated configuration module.

The configuration is available as app.conf:

>>> app.conf.CELERY_TIMEZONE
'Europe/London'

app.config_from_object('celeryconfig')

The celeryconfig module may then look like this:

celeryconfig.py:
CELERY_ENABLE_UTC = True
CELERY_TIMEZONE = 'Europe/London'


When a task is retried this is also recorded as a task state, so that you can track the progress of the task using the result instance (see States).

Here’s an example using retry:

@app.task(bind=True)
def send_twitter_status(self, oauth, tweet):
    try:
        twitter = Twitter(oauth)
        twitter.update_status(tweet)
    except (Twitter.FailWhaleError, Twitter.LoginError) as exc:
        raise self.retry(exc=exc)


***STATES****
PENDING

Task is waiting for execution or unknown. Any task id that is not known is implied to be in the pending state.
STARTED

Task has been started. Not reported by default, to enable please see app.Task.track_started.
metadata:   pid and hostname of the worker process executing the task.
SUCCESS

Task has been successfully executed.
metadata:   result contains the return value of the task.
propagates: Yes
ready:  Yes
FAILURE

Task execution resulted in failure.
metadata:   result contains the exception occurred, and traceback contains the backtrace of the stack at the point when the exception was raised.
propagates: Yes
RETRY

Task is being retried.
metadata:   result contains the exception that caused the retry, and traceback contains the backtrace of the stack at the point when the exceptions was raised.
propagates: No
REVOKED

Task has been revoked.
propagates: Yes

*CALLING TASKS*



Quick Cheat Sheet

    T.delay(arg, kwarg=value)

        always a shortcut to .apply_async.

    T.apply_async((arg, ), {'kwarg': value})

    T.apply_async(countdown=10)

        executes 10 seconds from now.

    T.apply_async(eta=now + timedelta(seconds=10))

        executes 10 seconds from now, specifed using eta

    T.apply_async(countdown=60, expires=120)

        executes in one minute from now, but expires after 2 minutes.

    T.apply_async(expires=now + timedelta(days=2))

        expires in 2 days, set using datetime.



Celery supports linking tasks together so that one task follows another. The callback task will be applied with the result of the parent task as a partial argument:
add.apply_async((2, 2), link=add.s(16)
Here the result of the first task (4) will be sent to a new task that adds 16 to the previous result, forming the expression

ETA anc countdown
>>> result = add.apply_async((2, 2), countdown=3)
>>> result.get()    # this takes at least 3 seconds to return

Expiry
The expires argument defines an optional expiry time, either as seconds after task publish, or a specific date and time using datetime:
>>> # Task expires after one minute from now.
>>> add.apply_async((10, 10), expires=60)

Retries:
Celery will automatically retry sending messages in the event of connection failure, and retry behavior can be configured – like how often to retry, or a maximum number of retries – or disabled all together.

To disable retry you can set the retry execution option to False:

add.apply_async((2, 2), retry=False)

Serialization:
Data transferred between clients and workers needs to be serialized, so every message in Celery has a content_type header that describes the serialization method used to encode it.
Example setting a custom serializer for a single task invocation:
>>> add.apply_async((10, 10), serializer='json')


Compression:
Example specifying the compression used when calling a task:
>>> add.apply_async((2, 2), compression='zlib'



Canvas:
A signature() wraps the arguments, keyword arguments, and execution options of a single task invocation in a way such that it can be passed to functions or even serialized and sent across the wir
Signatures are often nicknamed “subtasks” because they describe a task to be called within a task.

Method1
>>> signature('tasks.add', args=(2, 2), countdown=10)
tasks.add(2, 2)
Method2
>>> add.subtask((2, 2), countdown=10)
tasks.add(2, 2)
Method3
There is also a shortcut using star arguments:
>>> add.s(2, 2)
tasks.add(2, 2

It supports the “Calling API” which means it supports delay and apply_async or being called directly.
>>> add.apply_async((2, 2), countdown=1)
>>> add.subtask((2, 2), countdown=1).apply_async()

Partials

With a signature, you can execute the task in a worker:
>>> add.s(2, 2).delay()
>>> add.s(2, 2).apply_async(countdown=1)

Any arguments added will be prepended to the args in the signature:
>>> partial = add.s(2)          # incomplete signature
>>> partial.delay(4)            # 2 + 4

or
Any options added will be merged with the options in the signature, with the new options taking precedence:
>>> s = add.subtask((2, 2), countdown=10)
>>> s.apply_async(countdown=1)  # countdown is now 1


Immutability
Partials are meant to be used with callbacks, any tasks linked or chord callbacks will be applied with the result of the parent task. Sometimes you want to specify a callback that does not take additional arguments, and in that case you can set the signature to be immutable:
>>> add.apply_async((2, 2), link=reset_buffers.subtask(immutable=True))

Callback
Callbacks can be added to any task using the link argument to apply_async:
add.apply_async((2, 2), link=other_task.s())
The callback will only be applied if the task exited successfully, and it will be applied with the return value of the parent task as argument.

Chains:


Here’s a simple chain, the first task executes passing its return value to the next task in the chain, and so on.
>>> from celery import chain
# 2 + 2 + 4 + 8
>>> res = chain(add.s(2, 2), add.s(4), add.s(8))()
>>> res.get()
16
This can also be written using pipes:
>>> (add.s(2, 2) | add.s(4) | add.s(8))().get()
16


Immutabel sgnatures

Signatures can be partial so arguments can be added to the existing arguments, but you may not always want that, for example if you don’t want the result of the previous task in a chain.
In that case you can mark the signature as immutable, so that the arguments cannot be changed:
>>> add.subtask((2, 2), immutable=True)
There’s also an .si shortcut for this:
>>> add.si(2, 2)
Now you can create a chain of independent tasks instead:
>>> res = (add.si(2, 2) | add.si(4, 4) | add.s(8, 8))()
>>> res.get()
16
>>> res.parent.get()
8
>>> res.parent.parent.get()
    4


Simple group
    You can easily create a group of tasks to execute in parallel:

    >>> from celery import group
    >>> res = group(add.s(i, i) for i in xrange(10))()
    >>> res.get(timeout=1)
    [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]

Simple Chords
The chord primitive enables us to add callback to be called when all of the tasks in a group have finished executing, which is often required for algorithms that aren’t embarrassingly parallel:

>>> from celery import chord
>>> res = chord((add.s(i, i) for i in xrange(10)), xsum.s())()
>>> res.get()
90
From 3.1 errors will propagate to the callback, so the callback will not be executed instead the callback changes to failure state, and the error is set to the ChordError exception:


Chunks

Chunking lets you divide an iterable of work into pieces, so that if you have one million objects, you can create 10 tasks with hundred thousand objects each.




***WORKERS***

The broadcast() function.

This is the client function used to send commands to the workers. Some remote control commands also have higher-level interfaces using broadcast() in the background, like rate_limit() and ping().

Sending the rate_limit command and keyword arguments:

>>> app.control.broadcast('rate_limit',
...                          arguments={'task_name': 'myapp.mytask',
...                                     'rate_limit': '200/m'})

Changing rate-limits at runtime
Example changing the rate limit for the myapp.mytask task to execute at most 200 tasks of that type every minute:
>>> app.control.rate_limit('myapp.mytask', '200/m')



revoking:
app.control.revoke('d9078da5-9915-40a0-bfa1-392c7bde42ed')
>>> app.control.revoke([
...    '7993b0aa-1f0b-4780-9af0-c47c0858b3f2',
...    'f565793e-b041-4b2b-9ca4-dca22762a55d',
...    'd9d35e03-2997-42d0-a13e-64a66b88a618',
])


The time limit (–time-limit) is the maximum number of seconds a task may run before the process executing it is terminated and replaced by a new process. 
>>> app.control.time_limit('tasks.crawl_the_web',
                           soft=60, hard=120, reply=True)
[{'worker1.example.com': {'ok': 'time limits set successfully'}}]

OR
>>> app.control.rate_limit('myapp.mytask', '200/m',
...            destination=['celery@worker1.example.com'])

You can specify what queues to consume from at startup, by giving a comma separated list of queues to the -Q option:

$ celery -A proj worker -l info -Q foo,bar,baz

Dynamically u can control the worker cosumption using
>>> app.control.add_consumer(
...     queue='baz',
...     exchange='ex',
...     exchange_type='topic',
...     routing_key='media.*',
...     options={
...         'queue_durable': False,
...         'exchange_durable': False,
...     },
...     reply=True,
...     destination=['w1@example.com', 'w2@example.com'])


Below gives stats
celery -A proj inspect stats





**ROUTING***
With this setting on, a named queue that is not already defined in CELERY_QUEUES will be created automatically. This makes it easy to perform simple routing tasks.

Say you have two servers, x, and y that handles regular tasks, and one server z, that only handles feed related tasks. You can use this configuration:

CELERY_ROUTES = {'feed.tasks.import_feed': {'queue': 'feeds'}}

Creating Queues in Celery:


In Celery available queues are defined by the CELERY_QUEUES setting.

Here’s an example queue configuration with three queues; One for video, one for images and one default queue for everything else:

default_exchange = Exchange('default', type='direct')
media_exchange = Exchange('media', type='direct')

CELERY_QUEUES = (
    Queue('default', default_exchange, routing_key='default'),
    Queue('videos', media_exchange, routing_key='media.video'),
    Queue('images', media_exchange, routing_key='media.image')
)
CELERY_DEFAULT_QUEUE = 'default'
CELERY_DEFAULT_EXCHANGE = 'default'
CELERY_DEFAULT_ROUTING_KEY = 'default'


