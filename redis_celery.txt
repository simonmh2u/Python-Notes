REDIS
Redis supports five different data structures: strings, hashes, lists, sets and ordered sets

Strings:
SET pages:about "about us"
GET pages:about
or 

127.0.0.1:6379> set  number:1 1
OK
127.0.0.1:6379> get number:1
"1"

DEL , TYPE ,EXISTS, INCR , INCRBY , DECR DECRBY , MGET(multiple) , MSET (multiple),GETSET(get old value set new value) are other commands 

> exists mykey
(integer) 1
> set mykey x
OK
> del mykey
(integer) 1

> mset a 10 b 20 c 30
OK
> mget a b c
1) "10"
2) "20"
3) "30

> incr counter
(integer) 102
> incrby counter 50
(integer) 152

> expire key 5 # key gets deleted after 5 seconds
(integer) 1

> set key 100 ex 10 # setting expiry in one command
ttl key is used to find the expory tmie



****List:*****
PUsh commands
> rpush mylist A
(integer) 1
> rpush mylist B
(integer) 2
> lpush mylist first
(integer) 3
> lrange mylist 0 -1  (-1 is the last element index just like pythonlists)
1) "first"
2) "A"
3) "B

On the contrary rpop, lpop are the commnds used to remove elements from cache.

Lists are useful for a number of tasks, two very representative use cases are the following:
# Remember the latest updates posted by users into a social network.
# Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, 
and a consumer (usually a worker) consumes those items executing actions. Redis has special list commands to make 
this use case both more reliable and efficient.

ltrim mylist 0 2 will just trip the list to the specific index size and discard remainging elements.
brpop , blpop are blocking commands i.e they wait on the queue if its empty until a producter pushes somethin onto them.

*When we add an element to an aggregate data type, if the target key does not exist, an empty aggregate data type is crated before
 adding the element and hence subsequent commands shuold match the data type created.

*When we remove elements from an aggregate data type, if the value remains empty, the key is automatically destroyed.
Lists let you associate an array of values to a single key. In fact, you can (and should) think of them as dynamic arrays. You can insert, append, pop, push, trim and so on.


**HASHES***

Hash:The hash data structure is exactly what you are thinking it is (a hash/dictionary).
>hmset user:1000 username antirez birthyear 1977 verified 1
OK
>   user:1000 username
"antirez"

Other commands :
hincrby user:1000 birthyear 10



****Sets:*****
Sets are a lot like lists, except they provide set-semantics (no duplicate values in a given set). You can diff sets via SDIFF, union two sets via SUNION or SUNIONSTORE .
> sadd myset 1 2 3
(integer) 3
> smembers myset
1. 3
2. 1
3. 2

> sismember myset 3  #true if present in set
(integer) 1
> sismember myset 30
(integer) 0

Sets are good for expressing relations between objects
Simple way to model this problem is to have a set for every object we want to tag. The set contains the IDs of the tags associated with the object.
Imagine we want to tag news. If our news ID 1000 is tagged with tag 1,2,5 and 77, we can have one set associating our tag IDs with the news:
> sadd news:1000:tags 1 2 5 77
(integer) 4
> sadd tag:1:news 1000
(integer) 1
> sadd tag:2:news 1000
(integer) 1
> sadd tag:5:news 1000
(integer) 1
> sadd tag:77:news 1000
(integer) 1

sinter tag:1:news tag:2:news tag:10:news tag:27:news # Gives all news items with mentioned tags
Sinter is the intersection between sets.Intersection is not the only operation performed, you can also perform unions, difference, extract a random element, and so forth.
scard returns number if elements in a set.



Sorted Sets;
It's wrong to call one structure more powerful than the others, since they all tend to serve fairly distinct needs,
 but sorted sets are pretty awesome. A sorted set is similar to a set, except each value is associated (and sorted by) a score field.
Sorted sets are a data type which is similar to a mix between asSet and an hash. Like sets, sorted sets are composed of unique, non-repeating string elements, so in some sense a sorted set is a set as well.
> zadd hackers 1940 "Alan Kay"
(integer) 1
> zadd hackers 1957 "Sophie Wilson"
(integer 1)

zrange hackers 0 -1
zrevrange hackers 0 -1
zrange hackers 0 -1 withscores

zrangebyscore hackers -inf 1950
1) "Alan Turing"
2) "Hedy Lamarr"
3) "Claude Shannon"
4) "Alan Kay"
5) "Anita Borg"




Model for Twitter : 

users hash containing userame and password -- when the user is created add entries in this hash.
somethign like 
set user_id 1000
incr user_id
users:user_id username "Simon" password "pass"

following sorted set  -- add the userd ids of users you follow when  you clik the folow button
somethign like
following:user_id time() 999 time() 1023 # time is the score used here
followers sorted set -- add the user ids of users that are following you
following:user_id time() 888 time() 0988 #time is the score

When user posts something by clicking post button we ad it into post hash something like.
post:post_id time time() userid user_id status "Message"

You also go ahead and add this post into posts list of all the users follwing you (called as Fanout)
posts:user_id post_id1,post_id2 ....



REDIS Python Binding :
r = redis.StrictRedis(host='localhost', port=6379, db=0)
r.set('foo','bar')
r.get('foo')

By default, each Redis instance you create will in turn create
its own connection pool. You can override this behavior and use an existing
connection pool by passing an already created connection pool instance to the
connection_pool argument of the Redis class

 >>> pool = redis.ConnectionPool(host='localhost', port=6379, db=0)
 >>> r = redis.Redis(connection_pool=pool)
 
U can use unix domain sockets as below
r = redis.Redis(unix_socket_path='/tmp/redis.sock')


Parser classes provide a way to control how responses from the Redis server
are parsed. redis-py ships with two parser classes, the PythonParser and the
HiredisParser. By default, redis-py will attempt to use the HiredisParser if
you have the hiredis module installed and will fallback to the PythonParser
otherwise

Pipelines are a subclass of the base Redis class that provide support for
buffering multiple commands to the server in a single request. They can be used
to dramatically increase the performance of groups of commands by reducing the
number of back-and-forth TCP packets between the client and server.


    >>> r = redis.Redis(...)
    >>> r.set('bing', 'baz')
    >>> # Use the pipeline() method to create a pipeline instance
    >>> pipe = r.pipeline()
    >>> # The following SET commands are buffered
    >>> pipe.set('foo', 'bar')
    >>> pipe.get('bing')
    >>> # the EXECUTE call sends all buffered commands to the server, returning
    >>> # a list of responses, one for each command.
    >>> pipe.execute()

They can be chained as below:
pipe.set('foo', 'bar').sadd('faz', 'baz').incr('auto_number').execute()


redis-py includes a `PubSub` object that subscribes to channels and listens
for new messages. Creating a `PubSub` object is 
>>> p.subscribe('my-first-channel', 'my-second-channel', ...)
>>> p.psubscribe('my-*', ...)

>>> p.get_message()
{'pattern': None, 'type': 'subscribe', 'channel': 'my-second-channel', 'data': 1L}

>>> r.publish('my-channel','Hellp')
>>> p.get_message()
{'channel': 'my-channel', data': 'Hellp', 'pattern': None, 'type': 'message'}

Useful code block for pubsub
   >>> while True:
    >>>     message = p.get_message()
    >>>     if message:
    >>>         # do something with the message
    >>>     time.sleep(0.001)



Some examples:

>>> import redis
>>> r_server = redis.Redis("localhost")


Data structure: String
>>> r_server.set("name", "DeGizmo")
True
>>> r_server.get("name")
'DeGizmo'

Integer
>>> r_server.set("hit_counter", 1)
True
>>> r_server.incr("hit_counter")
2
>>> r_server.get("hit_counter")
'2'

Lists:
>>> r_server.rpush("members", "Carol")
True
>>> r_server.lrange("members", 0, -1)
['Adam', 'Bob', 'Carol']
>>> r_server.llen("members")
3
>>> r_server.lindex("members", 1)
'Bob'
>>> r_server.rpop("members")
'Carol'
>>> r_server.lrange("members", 0, -1)
['Adam', 'Bob']
>>> r_server.lpop("members")
'Adam'
>>> r_server.lrange("members", 0, -1)
['Bob']


Set;
>>> r_server.sadd("members", "Carol")
True
>>> r_server.sadd("members", "Adam")
False
>>> r_server.smembers("members")
set(['Bob', 'Adam', 'Carol'])


Sorted Sets:

>>> r_server.zadd("stories:frontpage", "storyid:3123", 34)
True
>>> r_server.zadd("stories:frontpage", "storyid:9001", 3)
True
>>> frontpage = r_server.zrange("stories:frontpage", 0, -1, withscores=True)
>>> frontpage.reverse()
>>> frontpage
[('storyid:2134', 127.0), ('storyid:3123', 34.0), ('storyid:9001', 3.0)]


slaveoff is the config used for relpication which , the slave pulls data frmo master ans syncs it self.
usually slave is read only and only master takes writes.




***************CELERY**********************

GEneral Async Use Cases for Celery:
*geenrating assets after upload , like resigin pics etc.
*notifying users when an even t happends like , when somone start to follow you , send email
*Keeping search index uptodate
*replaceing cron jobs.


Mesage broker stores tasks on queues , allows workers on diff machne/proces to produce consume tasks.
The broker stors the data that answers the question ' What work remains to be done ?'
Result Backend: To track state of the task and the return values once the task is complete.

producer --> Broker --> consumer -> workers --> result backend (except broker and result backend all components are part of celery)

Dont pass object instances to calery tasks.
For concurrency use multiprocessnig
profile stuff and break down tasks.




Letâ€™s create the file tasks.py:

from celery import Celery

app = Celery('tasks', broker='amqp://guest@localhost//')

@app.task
def add(x, y):
    return x + y

The first argument to Celery is the name of the current module, this is needed so that names can be automatically generated, the second argument is the broker keyword argument which specifies the URL of the message broker you want to use, using RabbitMQ here,

You now run the worker by executing our program with the worker argument
$ celery -A tasks worker --loglevel=info
In production you will want to run the worker in the background as a daemo

To call our task you can use the delay() method.
>>> from tasks import add
>>> add.delay(4, 4)
Calling a task returns an AsyncResult instance which can be used to check the state of the task, wait for the task to finish or get its return value 


If you want to keep track of the tasksâ€™ states, Celery needs to store or send the states somewhere.
For this example you will use the amqp result backend, which sends states as messages. The backend is specified via the backend argument to Celery, (or via the CELERY_RESULT_BACKEND setting if you choose to use a configuration module):

app = Celery('tasks', backend='amqp', broker='amqp://')

The ready() method returns whether the task has finished processing or not:
>>> result.ready()
False

You can wait for the result to complete, but this is rarely used since it turns the asynchronous call into a synchronous one:
>>> result.get(timeout=1)

If the task raised an exception you can also gain access to the original traceback:
>>> result.traceback



from celery import Celery

app = Celery('proj',
             broker='amqp://',
             backend='amqp://',
             include=['proj.tasks'])

# Optional configuration, see the application user guide.
app.conf.update(
    CELERY_TASK_RESULT_EXPIRES=3600,)

The include argument is a list of modules to import when the worker starts. You need to add our tasks module here so that the worker is able to find our tasks.
The default concurrency number(workers) is the number of CPUâ€™s on that machine (including cores), you can specify a custom number using -c option.


â€“ Events is an option that when enabled causes Celery to send monitoring messages (events) for actions occurring in the worker.

â€“ Queues is the list of queues that the worker will consume tasks from.

Using multi to start workers:

# Single worker with explicit name and events enabled.
$ celery multi start Leslie -E

# Pidfiles and logfiles are stored in the current directory
# by default.  Use --pidfile and --logfile argument to change
# this.  The abbreviation %N will be expanded to the current
# node name.
$ celery multi start Leslie -E --pidfile=/var/run/celery/%N.pid
                                --logfile=/var/log/celery/%N.log


# You need to add the same arguments when you restart,
# as these are not persisted anywhere.
$ celery multi restart Leslie -E --pidfile=/var/run/celery/%N.pid
                                 --logfile=/var/run/celery/%N.log

# To stop the node, you need to specify the same pidfile.
$ celery multi stop Leslie --pidfile=/var/run/celery/%N.pid

# 3 workers, with 3 processes each
$ celery multi start 3 -c 3
celery worker -n celery1@myhost -c 3
celery worker -n celery2@myhost -c 3
celery worker -n celery3@myhost -c 3

# start 3 named workers
$ celery multi start image video data -c 3
celery worker -n image@myhost -c 3
celery worker -n video@myhost -c 3
celery worker -n data@myhost -c 3

# specify custom hostname
$ celery multi start 2 --hostname=worker.example.com -c 3
celery worker -n celery1@worker.example.com -c 3
celery worker -n celery2@worker.example.com -c 3

# specify fully qualified nodenames
$ celery multi start foo@worker.example.com bar@worker.example.com -c 3

# Advanced example starting 10 workers in the background:
#   * Three of the workers processes the images and video queue
#   * Two of the workers processes the data queue with loglevel DEBUG
#   * the rest processes the default' queue.
$ celery multi start 10 -l INFO -Q:1-3 images,video -Q:4,5 data
    -Q default -L:4,5 DEBUG

# You can show the commands necessary to start the workers with
# the 'show' command:
$ celery multi show 10 -l INFO -Q:1-3 images,video -Q:4,5 data
    -Q default -L:4,5 DEBUG

# Additional options are added to each celery worker' comamnd,
# but you can also modify the options for ranges of, or specific workers

# 3 workers: Two with 3 processes, and one with 10 processes.
$ celery multi start 3 -c 3 -c:1 10
celery worker -n celery1@myhost -c 10
celery worker -n celery2@myhost -c 3
celery worker -n celery3@myhost -c 3

# can also specify options for named workers
$ celery multi start image video data -c 3 -c:image 10
celery worker -n image@myhost -c 10
celery worker -n video@myhost -c 3
celery worker -n data@myhost -c 3

# ranges and lists of workers in options is also allowed:
# (-c:1-3 can also be written as -c:1,2,3)
$ celery multi start 5 -c 3  -c:1-3 10
celery worker -n celery1@myhost -c 10
celery worker -n celery2@myhost -c 10
celery worker -n celery3@myhost -c 10
celery worker -n celery4@myhost -c 3
celery worker -n celery5@myhost -c 3

# lists also works with named workers
$ celery multi start foo bar baz xuzzy -c 3 -c:foo,bar,baz 10
celery worker -n foo@myhost -c 10
celery worker -n bar@myhost -c 10
celery worker -n baz@myhost -c 10
celery worker -n xuzzy@myhost -c 3



>>> add.delay(2, 2)

This method is actually a star-argument shortcut to another method called apply_async():
>>> add.apply_async((2, 2))
The latter enables you to specify execution options like the time to run (countdown), the queue it should be sent to and so on:
>>> add.apply_async((2, 2), queue='lopri', countdown=10)

The delay and apply_async methods return an AsyncResult instance, which can be used to keep track of the tasks execution state

*If you have a result backend configured you can retrieve the return value of a task:

>>> res = add.delay(2, 2)
>>> res.get(timeout=1)
4
You can find the taskâ€™s id by looking at the id attribute:
>>> res.id
d6b3aea2-fb9b-4ebc-8da4-848818db9114

So how does it know if the task has failed or not? It can find out by looking at the tasks state:
>>> res.state
'FAILURE'

A task can only be in a single state, but it can progress through several states. The stages of a typical task can be:
PENDING -> STARTED -> SUCCESS


The started state is a special state that is only recorded if the CELERY_TRACK_STARTED setting is enabled, or if the @task(track_started=True) option is set for the task.


*Routing*
The CELERY_ROUTES setting enables you to route tasks by name and keep everything centralized in one location:
app.conf.update(
    CELERY_ROUTES = {
        'proj.tasks.add': {'queue': 'hipri'},
    },
)
You can also specify the queue at runtime with the queue argument to apply_async:
>>> from proj.tasks import add
>>> add.apply_async((2, 2), queue='hipri')


*You can then make a worker consume from this queue by specifying the -Q option:
$ celery -A proj worker -Q hipri

Use below to see wht this worker is curretly doing.
*$ celery -A proj inspect active --destination=celery@example.com

*For example you can force workers to enable event messages (used for monitoring tasks and workers):
$ celery -A proj control enable_events
When events are enabled you can then start the event dumper to see what the workers are doing:
$ celery -A proj events --dump



